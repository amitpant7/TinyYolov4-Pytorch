{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-16T06:22:57.822028Z","iopub.status.busy":"2024-06-16T06:22:57.821646Z","iopub.status.idle":"2024-06-16T06:23:02.136802Z","shell.execute_reply":"2024-06-16T06:23:02.135560Z","shell.execute_reply.started":"2024-06-16T06:22:57.821997Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import cv2\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T06:23:02.140099Z","iopub.status.busy":"2024-06-16T06:23:02.139500Z","iopub.status.idle":"2024-06-16T06:23:02.213739Z","shell.execute_reply":"2024-06-16T06:23:02.212374Z","shell.execute_reply.started":"2024-06-16T06:23:02.140058Z"},"trusted":true},"outputs":[],"source":["NO_OF_ANCHOR_BOX = N = 3\n","\n","S = [13, 26, 52]  #Three output prediction Scales of Yolov3\n","\n","NO_OF_CLASS = C =  3\n","HEIGHT = H = 416\n","WIDTH = W = 416\n","SCALE = [32, 16, 8]\n","\n","\n","DEVICE =device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = batch_size = 16\n","\n","\n","CLASS = 1\n","BOX = 3\n","BATCH_SIZE = 4\n","# ANCHORS\n","IMG_SIZE = IMAGE_SIZE =  416\n","grid = 13\n","GRID1 = IMG_SIZE // 32\n","GRID2 = IMG_SIZE // 16\n","GRID3 = IMG_SIZE // 8\n","stride = 32\n","\n","STRIDE1 = 32\n","STRIDE2 = 16\n","STRIDE3 = 8\n","\n","W = H = 416\n","\n","#################\n","CONF_THRESHOLD = 0.5\n","IOU_THRESHOLD = 0.5\n","\n","#################\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T06:23:02.216186Z","iopub.status.busy":"2024-06-16T06:23:02.215667Z","iopub.status.idle":"2024-06-16T06:23:02.230144Z","shell.execute_reply":"2024-06-16T06:23:02.228779Z","shell.execute_reply.started":"2024-06-16T06:23:02.216144Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def convert_to_corners(bboxes):\n","    cx, cy, w, h = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n","    x1 = cx - w / 2\n","    y1 = cy - h / 2\n","    x2 = cx + w / 2\n","    y2 = cy + h / 2\n","    return torch.stack([x1, y1, x2, y2], dim=1)\n","\n","def intersection_over_union(bb1, bb2):\n","    # Convert center-width-height format to top-left and bottom-right format\n","    bboxes1 = convert_to_corners(bb1)\n","    bboxes2 = convert_to_corners(bb2)\n","\n","    # Calculate the coordinates of the intersection rectangles\n","    x_left = torch.max(bboxes1[:, 0], bboxes2[:, 0])\n","    y_top = torch.max(bboxes1[:, 1], bboxes2[:, 1])\n","    x_right = torch.min(bboxes1[:, 2], bboxes2[:, 2])\n","    y_bottom = torch.min(bboxes1[:, 3], bboxes2[:, 3])\n","\n","    # Calculate the intersection area\n","    intersection_width = torch.clamp(x_right - x_left, min=0)\n","    intersection_height = torch.clamp(y_bottom - y_top, min=0)\n","    intersection_area = intersection_width * intersection_height\n","\n","    # Calculate the area of each bounding box\n","    bb1_area = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n","    bb2_area = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n","\n","    # Calculate the IoU\n","    iou = intersection_area / (bb1_area + bb2_area - intersection_area)\n","\n","    return iou\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T06:23:59.348822Z","iopub.status.busy":"2024-06-16T06:23:59.348425Z","iopub.status.idle":"2024-06-16T06:23:59.386558Z","shell.execute_reply":"2024-06-16T06:23:59.385299Z","shell.execute_reply.started":"2024-06-16T06:23:59.348792Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","from torchvision.transforms import v2\n","from torchvision import tv_tensors\n","\n","from config import *\n","\n","\n","class WIDERFaceDataseti(Dataset):\n","\n","    def __init__(self, split, transforms=None):\n","        super().__init__()\n","        self.transforms = transforms\n","        self.dataset = torchvision.datasets.WIDERFace(\n","            root=\"./data/\", split=split, download=True\n","        )\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        img, annots = self.dataset[idx]\n","        img = tv_tensors.Image(img)\n","        bboxes = annots[\"bbox\"]\n","        labels = torch.ones(len(bboxes))\n","\n","        w = bboxes[:, 2]\n","        h = bboxes[:, 3]\n","\n","        cx = bboxes[:, 0] + 0.5 * w\n","        cy = bboxes[:, 1] + 0.5 * h\n","\n","        bboxes = torch.stack((cx, cy, w, h), dim=1)\n","\n","\n","        bboxes = tv_tensors.BoundingBoxes(\n","            bboxes, format=\"CXCYWH\", canvas_size=img.shape[-2:]\n","        )\n","\n","        sample = {\"image\": img, \"labels\": labels, \"bboxes\": bboxes}\n","\n","\n","\n","        if self.transforms is not None:\n","            sample = self.transforms(sample)\n","\n","        return sample['image'], sample['bboxes']\n","\n","\n","class FinalTranform(torch.nn.Module):\n","    # Retruns target in the shape [S, S, N, C+5] for every Scale,\n","    # So a tesor represtnation of target for all anchor boxes and all scale values .\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, sample):\n","        image = sample[\"image\"]\n","        bboxes = sample[\"bboxes\"]\n","        labels = sample[\"labels\"]\n","\n","        # building targets\n","        targets = []\n","\n","        # for every scale[13,26,52]:\n","\n","        for i in range(len(S)):\n","            to_exclude = []  # we won't assign same anchor box multiple times.\n","\n","            target = torch.zeros(S[i], S[i], N, 1 + 4 + C)  # S*S*N, 1+4+C\n","\n","            for bbox, label in zip(bboxes, labels):\n","                cx, cy = bbox[0] / SCALE[i], bbox[1] / SCALE[i]  # Float values\n","                pos = (int(cx), int(cy))\n","                bx, by = cx - int(cx), cy - int(cy)\n","                box_width, box_height = bbox[2] / SCALE[i], bbox[3] / SCALE[i]\n","\n","                assigned_anchor_box, ignore_indices = match_anchor_box(\n","                    box_width, box_height, i, to_exclude\n","                )\n","\n","                if assigned_anchor_box is None:\n","                    continue\n","\n","                anchor_box = ANCHOR_BOXES[i][assigned_anchor_box]\n","\n","                bw_by_Pw, bh_by_ph = (\n","                    box_width / anchor_box[0],\n","                    box_height / anchor_box[1],\n","                )\n","\n","                epsilon = 1e-6\n","\n","                target[pos[0], pos[1], assigned_anchor_box, 0:5] = torch.tensor(\n","                    [\n","                        1,\n","                        bx,\n","                        by,\n","                        torch.log(bw_by_Pw + epsilon),\n","                        torch.log(bh_by_ph + epsilon),\n","                    ]\n","                )\n","                target[pos[0], pos[1], assigned_anchor_box, 5 + int(label)] = 1\n","\n","                to_exclude.append(assigned_anchor_box)\n","\n","                try:\n","                    for value in ignore_indices:\n","                        target[pos[0], pos[1], value.item(), 0] = -1\n","                except:\n","                    pass\n","\n","            targets.append(target)\n","\n","        return image, targets\n","\n","\n","def match_anchor_box(\n","    bbox_w,\n","    bbox_h,\n","    i,\n","    to_exclude=[],\n","):\n","    \"\"\"\n","    Matches the bounding box to the closest anchor box.\n","\n","    Parameters:\n","    - bbox_w (float): The width of the bounding box.\n","    - bbox_h (float): The height of the bounding box.\n","    - to_exclude (list): List of anchor boxes to exclude.\n","\n","    Returns:\n","    - int: Index of the matched anchor box.\n","    \"\"\"\n","    ignore = 0.5\n","    anchor_boxes = ANCHOR_BOXES[i]\n","    iou = []\n","    for i, box in enumerate(anchor_boxes):\n","        if i in to_exclude:\n","            iou.append(0)\n","            continue\n","        intersection_width = min(box[0], bbox_w)  # Scale up as h, w in range 0-13\n","        intersection_height = min(box[1], bbox_h)\n","        I = intersection_width * intersection_height\n","        IOU = I / (bbox_w * bbox_h + box[0] * box[1] - I)\n","        iou.append(IOU)\n","\n","    iou = torch.tensor(iou)\n","    best = torch.argmax(iou, dim=0).item()\n","    # I want to not assign anchor if the IOU is below this.\n","\n","    # if iou[best] < 0.1:\n","    #     best = None\n","\n","    # Ignore anchors if they have high IOU but are not the best match\n","    ignore_indices = torch.nonzero((iou > ignore) & (iou != iou[best])).squeeze()\n","\n","    return best, ignore_indices\n","\n","\n","def inverse_target(ground_truths, S=S, SCALE=SCALE, anchor_boxes=ANCHOR_BOXES):\n","    \"\"\"\n","    Converts the target tensor back to bounding boxes and labels.\n","\n","    Parameters:\n","    - ground_truth (torch.Tensor): The ground truth tensor.\n","    - S (int, optional): The size of the grid. Default is 13.\n","    - SCALE (int, optional): The scale factor. Default is 32.\n","    - anchor_boxes (list, optional): List of anchor boxes. Default is None.\n","\n","    Returns:\n","    - tuple: (bbox, labels) where bbox are the bounding boxes and labels are the object labels.\n","    \"\"\"\n","\n","    # Each list element will have reversed targets, i.e ground truth bb\n","    all_bboxes = []\n","    all_labels = (\n","        []\n","    )  # Just for verifying all the targets are properly build, if they can be reversed then good.\n","\n","    for i, ground_truth in enumerate(ground_truths):  # multiple targets\n","        bboxes = []\n","        labels = []\n","        ground_truth = ground_truth.to(device)\n","        cx = cy = torch.tensor([i for i in range(S[i])], device=device)\n","\n","        ground_truth = ground_truth.permute(0, 3, 4, 2, 1)\n","        ground_truth[..., 1:2, :, :] += cx\n","        ground_truth = ground_truth.permute(0, 1, 2, 4, 3)\n","        ground_truth[..., 2:3, :, :] += cy\n","        ground_truth = ground_truth.permute((0, 3, 4, 1, 2))\n","\n","        ground_truth[..., 1:3] *= SCALE[i]\n","        ground_truth[..., 3:5] = torch.exp(ground_truth[..., 3:5])\n","        ground_truth[..., 3:5] *= anchor_boxes[i].to(device)\n","        ground_truth[..., 3:5] = ground_truth[..., 3:5] * SCALE[i]\n","\n","        bbox = ground_truth[ground_truth[..., 0] == 1][..., 1:5]\n","        labels = ground_truth[ground_truth[..., 0] == 1][..., 5]\n","\n","        all_bboxes.append(bbox)\n","        all_labels.append(labels)\n","\n","    return all_bboxes, all_labels\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T06:25:50.009444Z","iopub.status.busy":"2024-06-16T06:25:50.008940Z","iopub.status.idle":"2024-06-16T06:25:50.021150Z","shell.execute_reply":"2024-06-16T06:25:50.019824Z","shell.execute_reply.started":"2024-06-16T06:25:50.009402Z"},"trusted":true},"outputs":[],"source":["transformations = v2.Compose(\n","    [\n","        v2.RandomPhotometricDistort(p=0.3),\n","        v2.RandomHorizontalFlip(p=0.5),\n","        v2.RandomZoomOut(\n","            p=0.2, side_range=(1.0, 1.3), fill={tv_tensors.Image: (128, 128, 128)}\n","        ),\n","        #     v2.RandomIoUCrop(min_scale = 0.9, max_scale = 1, max_aspect_ratio=1.25, min_aspect_ratio=0.75),\n","        # #     v2.Resize((416,416), antialias=True),\n","        v2.RandomPerspective(distortion_scale=0.2, p=0.1),\n","        v2.RandomRotation(degrees=20),\n","        v2.RandomResizedCrop(size=(416, 416), scale=(0.9, 1), antialias=True),\n","        v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n","        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        # v2.SanitizeBoundingBoxes(),\n","    ]\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T06:25:52.268525Z","iopub.status.busy":"2024-06-16T06:25:52.268084Z","iopub.status.idle":"2024-06-16T06:25:53.213578Z","shell.execute_reply":"2024-06-16T06:25:53.211729Z","shell.execute_reply.started":"2024-06-16T06:25:52.268492Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2427it [00:00, 18080951.70it/s]\n","/opt/conda/lib/python3.10/site-packages/torchvision/datasets/utils.py:260: UserWarning: We detected some HTML elements in the downloaded file. This most likely means that the download triggered an unhandled API response by GDrive. Please report this to torchvision at https://github.com/pytorch/vision/issues including the response:\n","\n","<!DOCTYPE html><html><head><title>Google Drive - Virus scan warning</title><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"/><style nonce=\"q-oeh9M2Or8srkiLncpDqg\">.goog-link-button{position:relative;color:#15c;text-decoration:underline;cursor:pointer}.goog-link-button-disabled{color:#ccc;text-decoration:none;cursor:default}body{color:#222;font:normal 13px/1.4 arial,sans-serif;margin:0}.grecaptcha-badge{visibility:hidden}.uc-main{padding-top:50px;text-align:center}#uc-dl-icon{display:inline-block;margin-top:16px;padding-right:1em;vertical-align:top}#uc-text{display:inline-block;max-width:68ex;text-align:left}.uc-error-caption,.uc-warning-caption{color:#222;font-size:16px}#uc-download-link{text-decoration:none}.uc-name-size a{color:#15c;text-decoration:none}.uc-name-size a:visited{color:#61c;text-decoration:none}.uc-name-size a:active{color:#d14836;text-decoration:none}.uc-footer{color:#777;font-size:11px;padding-bottom:5ex;padding-top:5ex;text-align:center}.uc-footer a{color:#15c}.uc-footer a:visited{color:#61c}.uc-footer a:active{color:#d14836}.uc-footer-divider{color:#ccc;width:100%}.goog-inline-block{position:relative;display:-moz-inline-box;display:inline-block}* html .goog-inline-block{display:inline}*:first-child+html .goog-inline-block{display:inline}sentinel{}</style><link rel=\"icon\" href=\"//ssl.gstatic.com/docs/doclist/images/drive_2022q3_32dp.png\"/></head><body><div class=\"uc-main\"><div id=\"uc-dl-icon\" class=\"image-container\"><div class=\"drive-sprite-aux-download-file\"></div></div><div id=\"uc-text\"><p class=\"uc-warning-caption\">Google Drive can't scan this file for viruses.</p><p class=\"uc-warning-subcaption\"><span class=\"uc-name-size\"><a href=\"/open?id=15hGDLhsx8bLgLcIRD5DhYt5iBxnjNF1M\">WIDER_train.zip</a> (1.4G)</span> is too large for Google to scan for viruses. Would you still like to download this file?</p><form id=\"download-form\" action=\"https://drive.usercontent.google.com/download\" method=\"get\"><input type=\"submit\" id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" value=\"Download anyway\"/><input type=\"hidden\" name=\"id\" value=\"15hGDLhsx8bLgLcIRD5DhYt5iBxnjNF1M\"><input type=\"hidden\" name=\"export\" value=\"download\"><input type=\"hidden\" name=\"confirm\" value=\"t\"><input type=\"hidden\" name=\"uuid\" value=\"88bdda64-38ea-4236-a713-38027eb7f667\"></form></div></div><div class=\"uc-footer\"><hr class=\"uc-footer-divider\"></div></body></html>\n","  warnings.warn(\n"]},{"ename":"RuntimeError","evalue":"The MD5 checksum of the download file ./data/widerface/WIDER_train.zip does not match the one on record.Please delete the file and try again. If the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mWIDERFaceDataseti\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mWIDERFaceDataseti.__init__\u001b[0;34m(self, split, transforms)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWIDERFace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/widerface.py:67\u001b[0m, in \u001b[0;36mWIDERFace.__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m verify_str_arg(split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download and prepare it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/widerface.py:184\u001b[0m, in \u001b[0;36mWIDERFace.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# download and extract image data\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (file_id, md5, filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFILE_LIST:\n\u001b[0;32m--> 184\u001b[0m     \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, filename)\n\u001b[1;32m    186\u001b[0m     extract_archive(filepath)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/utils.py:268\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[1;32m    260\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    261\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe detected some HTML elements in the downloaded file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis most likely means that the download triggered an unhandled API response by GDrive. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report this to torchvision at https://github.com/pytorch/vision/issues including \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_md5(fpath, md5):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MD5 checksum of the download file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match the one on record.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease delete the file and try again. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n","\u001b[0;31mRuntimeError\u001b[0m: The MD5 checksum of the download file ./data/widerface/WIDER_train.zip does not match the one on record.Please delete the file and try again. If the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues."]}],"source":["train_data = WIDERFaceDataseti(split='train',\n","                            transforms=transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:31.783379Z","iopub.status.busy":"2024-05-23T09:53:31.783074Z","iopub.status.idle":"2024-05-23T09:53:32.682591Z","shell.execute_reply":"2024-05-23T09:53:32.681516Z","shell.execute_reply.started":"2024-05-23T09:53:31.783353Z"},"trusted":true},"outputs":[],"source":["all_boxes = []\n","for sample in train_data:\n","    all_boxes+=list(sample[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:32.68501Z","iopub.status.busy":"2024-05-23T09:53:32.684373Z","iopub.status.idle":"2024-05-23T09:53:33.100431Z","shell.execute_reply":"2024-05-23T09:53:33.099203Z","shell.execute_reply.started":"2024-05-23T09:53:32.684969Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","\n","fig, ax = plt.subplots()\n","\n","train_boxes = []\n","for batch in all_boxes:\n","    for box in batch:\n","        x1, y1, x2, y2 = box\n","        rect = Rectangle((x1-x2/2, y1-y2/2), x2, y2, edgecolor='red', facecolor='none', alpha=0.2)\n","        ax.add_patch(rect)\n","        train_boxes.append([x1,y1,x2,y2])\n","\n","ax.set_xlim(0, 416)  \n","ax.set_ylim(0, 416)  \n","ax.invert_yaxis()    \n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:33.102775Z","iopub.status.busy":"2024-05-23T09:53:33.102038Z","iopub.status.idle":"2024-05-23T09:53:33.113992Z","shell.execute_reply":"2024-05-23T09:53:33.112873Z","shell.execute_reply.started":"2024-05-23T09:53:33.102737Z"},"trusted":true},"outputs":[],"source":["def calculate_iou(box1, box2, image_size):\n","    ## Represent boxes at top left and bottom right coordinates\n","    x11,y11,w1,h1 = box1\n","#     x11,y11 = x11 * image_size[0], y11*image_size[1]\n","#     w1, h1 = w1 * image_size[0],h1 * image_size[1]\n","\n","    x21,y21,w2,h2 = box2\n","#     x21, y21 = x21 * image_size[0], y21 *image_size[1]\n","#     w2, h2 = w2 * image_size[0],h2 * image_size[1]\n","\n","    xtl1,ytl1 = x11-w1/2, y11-h1/2\n","    xbr1,ybr1 = x11+w1/2, y11+h1/2\n","\n","\n","    xtl2,ytl2 = x21-w2/2, y21-h2/2\n","    xbr2,ybr2 = x21+w2/2, y21+h2/2\n","\n","    x_inter_1 = max(xtl1,xtl2)\n","    y_inter_1 = max(ytl1,ytl2)\n","\n","    x_inter_2 = min(xbr1,xbr2)\n","    y_inter_2 = min(ybr1, ybr2)\n","\n","    width_inter = (x_inter_2-x_inter_1)\n","    height_inter = (y_inter_2-y_inter_1)\n","\n","    if width_inter<0 or height_inter<0:\n","        return 0\n","    area_inter = width_inter*height_inter\n","\n","    width_box1 = xbr1-xtl1\n","    height_box1 = ybr1-ytl1\n","    box_1_area = width_box1 * height_box1\n","\n","    width_box2 = xbr2-xtl2\n","    height_box2 = ybr2-ytl2\n","    box_2_area = width_box2* height_box2\n","\n","    area_union = box_1_area + box_2_area - area_inter\n","\n","    iou = area_inter / area_union\n","\n","    return iou"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:33.11967Z","iopub.status.busy":"2024-05-23T09:53:33.118621Z","iopub.status.idle":"2024-05-23T09:53:33.130518Z","shell.execute_reply":"2024-05-23T09:53:33.12924Z","shell.execute_reply.started":"2024-05-23T09:53:33.119625Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_set, val_set = train_test_split(train_boxes, test_size=0.2, random_state=42,shuffle=True)\n","print(\"train_set\", len(train_set))\n","print(\"val_set\", len(val_set))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:33.132366Z","iopub.status.busy":"2024-05-23T09:53:33.132045Z","iopub.status.idle":"2024-05-23T09:53:33.855928Z","shell.execute_reply":"2024-05-23T09:53:33.854851Z","shell.execute_reply.started":"2024-05-23T09:53:33.13234Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","\n","k_values = range(1, 15)\n","iou_avg = []\n","\n","for k in k_values:\n","    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n","    kmeans.fit(train_set)\n","    centroids = kmeans.cluster_centers_\n","\n","    iou_values = []\n","    assigned_clusters = []\n","\n","    for val in val_set:\n","        centroid_distances = [1 - calculate_iou(centroid, val, (416, 416)) for centroid in centroids]\n","        nearest_cluster_index = np.argmin(centroid_distances)\n","        assigned_clusters.append(nearest_cluster_index)\n","        iou_values.append(1 - centroid_distances[nearest_cluster_index])\n","\n","    mean_iou = np.mean(iou_values)\n","    iou_avg.append(mean_iou)\n","\n","    for centroid_index in range(len(centroids)):\n","        assigned_data_points = [val_set[i] for i, cluster_index in enumerate(assigned_clusters) if cluster_index == centroid_index]\n","        if assigned_data_points:\n","            new_centroid = np.mean(assigned_data_points, axis=0)\n","            centroids[centroid_index] = new_centroid\n","\n","plt.plot(k_values, iou_avg, marker='o')\n","plt.xlabel('Number of Clusters (k)')\n","plt.ylabel('Mean IOU')\n","plt.xticks(k_values)\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:46.087188Z","iopub.status.busy":"2024-05-23T09:53:46.086774Z","iopub.status.idle":"2024-05-23T09:53:46.1062Z","shell.execute_reply":"2024-05-23T09:53:46.104682Z","shell.execute_reply.started":"2024-05-23T09:53:46.087156Z"},"trusted":true},"outputs":[],"source":["kmeans = KMeans(n_clusters=9, random_state=42,n_init='auto')\n","kmeans.fit(train_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:55:08.89125Z","iopub.status.busy":"2024-05-23T09:55:08.89084Z","iopub.status.idle":"2024-05-23T09:55:08.8978Z","shell.execute_reply":"2024-05-23T09:55:08.896518Z","shell.execute_reply.started":"2024-05-23T09:55:08.891221Z"},"trusted":true},"outputs":[],"source":["print(centroids)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:55:03.846418Z","iopub.status.busy":"2024-05-23T09:55:03.846013Z","iopub.status.idle":"2024-05-23T09:55:04.262823Z","shell.execute_reply":"2024-05-23T09:55:04.261658Z","shell.execute_reply.started":"2024-05-23T09:55:03.846387Z"},"trusted":true},"outputs":[],"source":["centroids = kmeans.cluster_centers_\n","fig,ax = plt.subplots()\n","\n","fig, ax = plt.subplots()\n","\n","for i in centroids:\n","    x1,y1,x2,y2 = i\n","    rect = Rectangle((x1-x2/2, y1-y2/2), x2 , y2, edgecolor='red', facecolor='none', alpha=0.2)\n","    ax.add_patch(rect)\n","\n","ax.set_xlim(0, 416) \n","ax.set_ylim(0, 416)  \n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:55:48.152895Z","iopub.status.busy":"2024-05-23T09:55:48.152474Z","iopub.status.idle":"2024-05-23T09:55:48.160461Z","shell.execute_reply":"2024-05-23T09:55:48.15902Z","shell.execute_reply.started":"2024-05-23T09:55:48.152862Z"},"trusted":true},"outputs":[],"source":["xyxy = [[204.9835972,  221.60649912, 294.09578263, 312.73110721],\n"," [292.72444861, 218.55079433, 166.33437402, 175.1397013 ],\n"," [144.7808461,  178.77215698, 150.48080566, 164.62496094]]\n","\n","anchors_wh13 = []\n","anchors_wh24 = []\n","anchors_wh52 = []\n","\n","for xy in xyxy:\n","    width = xy[2]/416\n","    height = xy[3]/416\n","    anchors_wh13.append([width * 13,height*13])\n","    anchors_wh24.append([width * 26,height*26])\n","    anchors_wh52.append([width * 8,height*8])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:55:49.327311Z","iopub.status.busy":"2024-05-23T09:55:49.32693Z","iopub.status.idle":"2024-05-23T09:55:49.334232Z","shell.execute_reply":"2024-05-23T09:55:49.333053Z","shell.execute_reply.started":"2024-05-23T09:55:49.327281Z"},"trusted":true},"outputs":[],"source":["print(torch.tensor(anchors_wh13))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:55:50.609052Z","iopub.status.busy":"2024-05-23T09:55:50.60801Z","iopub.status.idle":"2024-05-23T09:55:50.616297Z","shell.execute_reply":"2024-05-23T09:55:50.615095Z","shell.execute_reply.started":"2024-05-23T09:55:50.609011Z"},"trusted":true},"outputs":[],"source":["print(torch.tensor(anchors_wh13)/2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:55:51.012117Z","iopub.status.busy":"2024-05-23T09:55:51.011715Z","iopub.status.idle":"2024-05-23T09:55:51.018877Z","shell.execute_reply":"2024-05-23T09:55:51.01767Z","shell.execute_reply.started":"2024-05-23T09:55:51.012067Z"},"trusted":true},"outputs":[],"source":["print(torch.tensor(anchors_wh13)/4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:34.352247Z","iopub.status.busy":"2024-05-23T09:53:34.35185Z","iopub.status.idle":"2024-05-23T09:53:34.360677Z","shell.execute_reply":"2024-05-23T09:53:34.359549Z","shell.execute_reply.started":"2024-05-23T09:53:34.352212Z"},"trusted":true},"outputs":[],"source":["print(anchors_wh24)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T09:53:34.362371Z","iopub.status.busy":"2024-05-23T09:53:34.362043Z","iopub.status.idle":"2024-05-23T09:53:34.372566Z","shell.execute_reply":"2024-05-23T09:53:34.371452Z","shell.execute_reply.started":"2024-05-23T09:53:34.362342Z"},"trusted":true},"outputs":[],"source":["print(anchors_wh52)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":34662,"sourceId":46346,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
